---
title: "Lab VAR models (Gali)"
author: "Pedro J. Perez"
date: "Marzo de 2019"
output: 
  html_document:
    highlight: haddock
    #theme: cerulean
    toc: yes
    toc_float: yes
---

```{r knitr_init, echo=FALSE, cache=FALSE, warning=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)

opts_knit$set(width=75)
```


--------------------   


# Lab on Applied VAR Modelling



--------------------  


1. Objective  
2. Gali (1999) model:  
     + Data preparation  
     + Model Specification  
     + Estimation of the VAR  
     + Validation of the VAR  
     
3. VAR in reduced form (uses):  
     + Granger causality  
     + Forecasting  
4. VMA representation  
     + Obtaining the VMA with R   
5. Uses of VAR: "structural" analysis:  
     + IRFs   
     + FEVD       
     
     
     
<br>
<br>

---------------------


# 1. Objective

<br>

* Lab to illustrate the possibilities of VAR modelling with R  


* We are going to use the R package `vars` written by Bernhard Pfaff. A short description of the functionalities of the Pfaff's package could be found [here](http://www.jstatsoft.org/v27/i04/paper).  For a more detailed exposition, please go [here](http://cran.r-project.org/web/packages/vars/vars.pdf)  


* To illustrate the different topics in VAR modelling, we are going to use as an example the analysis and data used in [Gali (1999)](http://crei.cat/people/gali/jgaer99.pdf) paper : "Technology, Employment, and the Business Cycle: Do Technology Shocks Explain Aggregate Fluctuations?   


<br>

-------------------------------   


# 2. Gali (1999) model

<br>

* In his [paper](http://crei.cat/people/gali/jgaer99.pdf), Gali estimates a bivariate VAR with productivity and hours to look mainly at the response of hours to a technology shock  


* In fact we are going to "replicate" the Gali paper but only his benchmark model:


    - U.S. quarterly  data for  **1948:1 - 1994:4** from Citibase    
  
    - bivariate VAR model: productivity($x_{t}$) and hours($n_{t}$)  
    
    - $y_{t~}=~\left[ x_{t~},n_{t}\right] ^{^{\prime }}$, both variables in (log) **first differences**    
  

<br>


To replicate in your PC the Gali analysis, copy and paste the following R script:  

  - Be aware  that you have to change the path to load the data. The `DATA_gali_99.csv` file with the data are in Aula virtual 
<br>

```{r, eval = F}
#- 1) dowload the DATA_gali_99.csv in you PC  
gali_data <- read.csv("./datos/DATA_gali_99.csv", sep = ";" , header = T)   #- change the path to the file   
#- 2) defining the ts variables   
GDP <- ts(gali_data$GDPQ, start = c(1947 ,1) , end = c(1994 , 4) , frequency = 4)         
Hours <- ts(gali_data$LPMHU, start = c(1947 ,1) , end = c(1994 , 4) , frequency = 4)    
Productivity <- GDP/Hours  
#- 3) setting the sample   
GDP <- window(GDP, start = c(1948 ,1) , end = c(1994 , 4))    
Hours <- window(Hours, start = c(1948 ,1) , end = c(1994 , 4))   
Productivity <- window(Productivity, start = c(1948 ,1) , end = c(1994 , 4))   
#- 4) Taking log's and indexing (1948:1 = 100) the variables    
lGDP <- 100+100*log(GDP/GDP[1])   
lHours <- 100+100*log(Hours/Hours[1])   
lProductivity <- 100+100*log(Productivity/Productivity[1])
#- 5) Taking first differences of the log-variables   
dy <- diff(lGDP,   lag=1, difference=1)   
dn <- diff(lHours, lag=1, difference=1)   
dx <- diff(lProductivity,   lag=1, difference=1)   
#- 6) Install & loading the vars package   
install.packages('vars')
library(vars)   
#- 7) Creating a matrix with the two series    
variables <- cbind(dx, dn)  
#- 8) Estimating the VAR   
VAR(variables, p = 4, type = "const")   
```

<br>

---------------------  

## Data preparation  

<br>

Step by step the analysis will be:  

- Let's start opening R and loading the Gali(99) data. The original data can be found at the [Estima web page](https://www.estima.com/cgi-bin/procbrowser.cgi?Version=900&Revision=Any&Subject=VAR&Reference=Any) (on the file galiaer1999.zip)  
 


```{r}
#----loading data(csv format with (;) for separator and with labels or header)
gali_data <- read.csv(here::here("datos", "DATA_gali_99.csv"), sep = ";" , header = TRUE )  
```  

 <br> 


 
- Let's see what variables are inside the data file  

```{r}
#-- str() displays the internal structure of an R object
str(gali_data[,1:8])   #-- only for the 1:8 columns of gali_data
```  
 
 <br> 

-  The data are quarterly and runs from 1947:1 to 1994:4  

- The `vars` package only works with data in a specific format: time series or ts. To convert our data in the time series format we are going to use the `ts()` function in R   

    - GDPQ is the measure of aggregate output  
    
    - LPMHU is labor input (hours)  
    
```{r}
#-- creating the time series with ts(): GDP, Hours & Productivity (labor productivity)
GDP <- ts(gali_data$GDPQ, start = c(1947 ,1) , end = c(1994 , 4) , frequency = 4)         
Hours <- ts(gali_data$LPMHU, start = c(1947 ,1) , end = c(1994 , 4) , frequency = 4)  
Productivity <- GDP/Hours
```

<br>

- We will start the analysis with the sample 1948:1 to 1994:4  


```{r}
#-- Changing the start date of the series to 1948:1 
GDP <- window(GDP, start = c(1948 ,1) , end = c(1994 , 4)) 
Hours <- window(Hours, start = c(1948 ,1) , end = c(1994 , 4))
Productivity <- window(Productivity, start = c(1948 ,1) , end = c(1994 , 4))
```


```{r}
#-- Taking log's and indexing (1948:1 = 100) the variables  
lGDP <- 100+100*log(GDP/GDP[1])
lHours <- 100+100*log(Hours/Hours[1])
lProductivity <- 100+100*log(Productivity/Productivity[1])
```

```{r}
#-- Taking first differences of the log-variables
dy <- diff(lGDP,   lag=1, difference=1)
dn <- diff(lHours, lag=1, difference=1)
dx <- diff(lProductivity,   lag=1, difference=1)
```

 <br> 

- Finally, the two variables used in the Gali's VAR were **dx** and **dy**: first difference of productivity and hours (both in logs) [growth rate's of labor productivity (**dx**) and hours (**dn**)]    

- That is, Hours and Productivity are supposed to be I(1) variables. Obviously, Gali checked this before   

- The VAR is estimated using data from 19**48**:1 - 1994:4 for the variables dx dn   
 
 <br>
 
- To estimate the VAR models we are going to use the R **vars package**, let's load the vars package   

```{r, echo=TRUE,results='hide',message=FALSE, warning=F}
#-- You will have first to install the vars packages with: install.packages("vars")
library(vars)
variables <- cbind(dx,dn)   #-- Creating a matrix with the two series
```

<br>

- We are now ready to estimate the VAR model: the vars package has a function, the **VAR()** function, that estimate VAR models by OLS   

- Simply writing in the R console the following: `VAR(variables, p = 4, type = "const")`, we will estimate a VAR model for the 2 variables in the matrix called *"variables"* with a constant and 4 lags(p), ... but, why 4 lags?   

<br>

- **How p should be fixed?** How to select the order of the VAR model?

<br>

------------  

##  Model Specification 

We have already decided which variables to include in our VAR model, the sample, if the variables are I(1) vs. I(0) , and the deterministic components . In this case we have almost finished the model specification. Almost, because ... we need to decide the order of the VAR.  

<br>

#### How to select (p) the order of the VAR?

For a more detailed exposition go to [LÃ¼tkepohl(2011)](http://cadmus.eui.eu/bitstream/handle/1814/19354/ECO_2011_30.pdf), pp. 10-11

- The idea is that we have to select an order(p) sufficiently large to ensure that the residuals shown no autocorrelation but without exhausting the degrees of freedom.  



- The order of the VAR could be selected by:   

     1) Sequential testing procedures  
     
     2) Model selection criteria




- The package "vars" has the  `VARselect()` function that allows to apply easily the 2^nd^ approach to select p.

- Remember that for viewing the syntax and options of an R function you could use the R `args()` function   

- If you need additional explanations you should call the R-help typing at the R console the following: `?VARselect`   

<br>

```{r, echo=TRUE, results=TRUE}
args(VARselect)   #-- VARselect() is a function of the vars package
``` 

```{r, echo=TRUE, results=TRUE}
variables <- cbind(dx,dn)   #-- Creating a matrix with the two series
VARselect(variables, lag.max = 8, type = "const")
``` 

<br>

- Three criteria (AIC,HQ &FPE) choose p=2;  BUT, as our data are quarterly, probably, as Gali does, a more sensible choice would be p=4   


<br>

 Finally, we are now ready to estimate our VAR with p=4. Let's go!!!    
 
 <br>

------------------------   

## Estimation of the VAR  

<br>

- **Estimating the VAR** model (in reduced form)  

```{r, echo=TRUE, results=TRUE}
#- we are saving the estimation results in the object "our_var"
our_var <- VAR(variables, p = 4, type = "const")   
``` 

<br>

- Let's see the estimation results. The best way is through the function `summary()`  

```{r, echo=TRUE}
summary(our_var)   
```  

<br>

- Let's see the (same) estimation results, but in other format:  the $A_{i}$ matrices   
 
```{r, echo=TRUE}
Acoef(our_var)   #-- shows the A estimated matrices. Be aware that they are the same results
```  

<br>

- After estimation and before to use or transform our VAR we have to check their validity, mainly looking at the residuals

<br>
<br>

-------------------  

## Validation of the VAR

<br>

  
   1) Stability and roots
   
   <br>
   
```{r}
#- Checking stability(eigenvalues of the companion coefficient matrix must have modulus less than 1)
roots(our_var, modulus=TRUE) #-- Returns a vector with the eigenvalues
````

 <br> 
 
  2) Accessing the Y-fitted, with the **fitted()** function
  
  <br> 
  
```{r}
#-- We can access fitted-Y with fitted()
print(fitted(our_var)[1:4, ])   #---- Y-fitted fot the first 4 observations
````

  <br>  
  
  3) Accessing the residuals
  
  <br>
  
```{r}
#-- We can access the residuals with residuals()
print(residuals(our_var)[1:4, ])  #-- print the four first residuals
````

<br>

  4) "Plotting" our VAR
  
  <br>
  
```{r}
#-- We can plot easily Y, Y-estimated and the residuals
plot(our_var)  
````

<br>

  5) Testing on the residuals
  
  <br>
  
```{r}
#-- We can test autocorrelation and normality of residuals
serial.test(our_var, lags.pt = 8 )  #-- 8 lags for autocorrelation portmanteau test
normality.test(our_var, multivariate.only=FALSE)   #-- Normality test
````

<br>

   6) "Plotting" our "residuals test"
   
   <br>
   
```{r}
#-- There is a function plot or method associated to the autocorrelation test
our_serial <- serial.test(our_var, lags.pt = 8) #-saving our autocorr.test in object "our_serial"
plot(our_serial)
````

<br>
<br>

------------------------   


# 3. VAR in reduced form (Uses)   

<br>

The two principal uses of VAR models in reduced form are testing (causality testing) and forecasting, BUT the most used instruments in the VAR methodology are IRF & FEVD.  

IRF & FEVD only have a clear meaning if we transform our reduced form VAR model to an structural VAR. We will develop this idea in a while  

<br>


### Testing in a VAR


- After validation, the VAR could be used for testing, for example, for testing Granger causality  


- As we said, if the residuals are normally distributed (Gaussian) like ($v_{t}\rightarrow N(0,\Sigma _{v})$) the **OLS estimator has desirable asymptotic properties**. In particular, it will be asymptotically normally distributed, and then, if the VAR is stable, usual inference procedures are asymptotically valid: t-statistics could be used for inference about individual parameters and F-test for testing hypothesis for sets of parameters.  

<br>

-------------------------

## Granger causality   

<br>

- **Granger causality**: Granger called a variable X causal for Y if the information in past and present values of X is helpful for improving the forecast of Y. If Granger causality holds, this does not guarantee that X causes Y. But, it suggests that X might be causing Y.

- In the context of VAR models, if we want to test for Granger-causality, we need to test zero constraints in some of the coefficients.  


- Sometimes econometricians use the shorter terms *causes* as shorthand for *Granger causes*. You should notice, however, that Granger causality is not causality in a deep sense of the word. It just talk about linear prediction, and it only has "teeth" if we only find Granger causality in one direction.  

- The definition of Granger causality did not mention anything about possible instantaneous correlation between variables. If the innovations are correlated we will say that there exits instantaneous causality  




* It's possible to test Granger causality through Wald or F-test. In the `vars` package:

    - It uses F-test to test Granger causality
   
    - Wald test for instantaneous Granger causality (non zero correlation among the $v_{it}$)

<br>

```{r}
#-- We can test Granger causality
causality(our_var, cause = c("dx"))    #-- dx Granger-cause the other(s) variables?
causality(our_var, cause = c("dn"))    #-- dn Granger-cause the other(s) variables?
````


<br>

-------------------------  

## Forecasting


- We are not going to develop this topic, but...     a VAR could also be used  for forecasting
  
```{r}
#-- We can use the VAR for forecasting
predict(our_var, n.ahead = 3)  
fanchart(predict(our_var))
````


<br>
------------------   


# 4. VMA represesentation  
<br>

* A stationary (stable) VAR could be transformed (inverted) in an infinite MA($\infty$) process

 

* By inverting the autoregressive polynomial A(L) we can obtain the VMA form of a VAR like:  

 
$$y_{t}~=C_{0}v_{t}+C_{1}v_{t-1}+C_{2}v_{t-2}+\ ...\  [3]$$   


Being $C_{0}=I_{K}$, the rest of the $C_{s}$ matrices could be computed recursively 

<br> 

* As with the VAR, model $[3]$ can be written using a polynomial in the lag operator as:
 


$$y_{t}=C(L)v_{t}\ \ \ \ \ \ \ \ \ \ \ \ [4]$$

being $C(L)=(I_{K}+C_{1}L^{1}+C_{2}L^{2}+\ \ ...)$.


<br>

* Model $[3]$ or $[4]$ are also called the Wold VMA representation  

<br>

------------------------  

## Obtaining the VMA with R    

<br>

First, test's do it by hand (but with a little help from R).


1) we could obtain the $A_{i}$ matrices of the estimated VAR with the `Acoef()`  function

```{r}
#--- Using the vars package to estimate the VAR
our_var <- VAR(variables, p = 4, type = "const")  #-- estimation of the VAR
AA <- Acoef(our_var) #-- saving the A matrices in AA
AA1 <- AA[[1]]    #-- AA1
AA2 <- AA[[2]]    #-- AA2
AA3 <- AA[[3]]    #-- AA3
AA4 <- AA[[4]]    #-- AA3
````

<br>

2) To obtain the Wold VMA representation of the VAR we have to invert the $A(L)$ polynomial. We will do it by hand and also by using the `vars` package  

<br>

2a) **By hand:** we could obtain the VMA matrices, the $C_{i}$, with R with the `Psi()` function; but by hand it would be as:
    
   - $C_{0}=I_{K}$,  
   
  - the rest of the $C_{s}$ matrices could be computed recursively as 
  
  $$C_{s}=\underset{j=1}{\overset{s}{\sum }}C_{s-j}A_{j}$$ 
  <!--- Esta eq. es inline y con sumatorio. No se puede -->
 
  
```{r}
#-------------------------obtaining the C(i) matrices of the VMA representation
CC0 <- diag(2)    #-- C(O) is the identity matrix (2x2)
CC1 <- CC0%*%AA1
CC2 <- CC1%*%AA1+CC0%*%AA2
CC3 <- CC2%*%AA1+CC1%*%AA2+CC0%*%AA3
CC4 <- CC3%*%AA1+CC2%*%AA2+CC1%*%AA3+CC0%*%AA4
CC5 <- CC4%*%AA1+CC3%*%AA2+CC2%*%AA3+CC1%*%AA4+CC0
````  

<br>
<br>

2b) **Using the vars package:** we have to use the `Phi()` function 

```{r}
#--- Wold VMA of our_var with the vars package
Phi(our_var, nstep = 3)  #- shows the WOLD VMA  estimated matrices. (by inverting the A's matrices)
````         

<br>

<br>

Let's check if our calculation by hand coincide with the one obtained using the vars package. Our (by hand) CC2 matrix is: 


```{r}
CC2   #-- our CC2 is nstep=3; that is in the vars package CCO = I is the first step
````

<br>
<br>

--------------------------  


# 5. Uses of VAR: "Structural" analysis

<br>

* The two principal instruments of VAR's, IRF & FEVD, are defined in terms of their VMA representation [3]   


* IRF & FEVD will show the effects of a shock, **BUT** in order to have a clear meaning, they must be interpreted under the assumption that all the other shocks are held constant;  however, in the Wold representation the shocks are not orthogonal; that's why we will turn our attention to the **Structural VAR models**   

<br>

-------------------------  

## IRFs

<br>

* Usually the main interest in VAR modelling is to look at the dynamic effect of a shock on the variables of interest


* This dynamic effect could be easily obtained through the VMA representation [3] of the VAR

* In particular the response of the variable $y_{n}$ to an impulse of size one in $v_{m}$ $j$-periods ahead is given by the $(n,m)$*-th* element of $C_{j}$. That is, the matrices $C_{i}$ contain the responses of the variables to the innovations for different periods (or steps) ahead   


#### BUT...


* BUT, usually the VAR disturbances (or innovations) are correlated, then the interpretability of the impulse responses to innovation becomes problematic: if the innovations are correlated (off diagonal elements of $\Sigma _{v}$ different from zero) then, an impulse in $v_{m}$ would be associated with impulses in innovations at the other equations of the VAR model.  

<br>

### Sims proposal: Cholesky decomposition

* Therefore, Sims proposed to assume recursive contemporaneous interactions among variables, i.e. imposing a certain structural ordering in the variables. In terms of the MVA representation this means that the transformed or "structural" shocks will not affect to the preceding variables instantaneously.    

<br>

* In practice, imposing a recursive contemporaneous order among the variables of the VAR model, is operationalised performing a Cholesky decomposition in $\Sigma _{v}$.  
 

* The Cholesky factor, $P$, of $\Sigma _{v}$ is defined as the unique lower triangular matrix such that $PP^{^{\prime }}=\Sigma _{v}$
    
<br>

* With the Cholesky factor($P$) we could transform the VAR in [3] as:


$$A(L)y_{t}=PP^{-1}v_{t}$$

with $\varepsilon _{t}=P^{-1}v_{t}$, then our transformed  VAR becomes 

$$A(L)y_{t}=P\varepsilon _{t}\ \ \ \ \ \ \ \ \ \ \ \ [2*]$$   


<br>


- That is, we have written our VAR in terms of a new vector of shocks $\varepsilon _{t}$, with identity covariance matrix ($\Sigma _{\varepsilon }=I$)   



* Now, as the $\varepsilon _{t}$ shocks are uncorrelated their IRF would have a clear interpretation   


<br>

--------------  


### Obtaining IRFs with R

<br>

* From [2*] we can obtain the VMA representation in terms of the $\varepsilon _{t}$ shocks:

$$y_{t}=C(L)P\varepsilon _{t}\ \ \ \ \ \ \ \ \ \ \ \ [5\ast ]$$


$$y_{t}=D(L)\varepsilon _{t}\ \ \ \ \ \ \ \ \ \ \ \ [5]$$

being $D(L)=C(L)P$ , 



$D(L)=(D_{0}+D_{1}L^{1}+D_{2}L^{2}+D_{3}L^{3}-\ \ ...)$,          with    $D_{i}=C_{i}P$ , 



then $D_{0}=C_{0}P=I_{N}P=P$



* As $D_{0}=P$, and P is lower triangular, the system is recursive: the first shock ($\varepsilon _{t}^{1}$) could have an instantaneous effect on all the variables of the VAR, while the first variable in the VAR could only be affected contemporaneously by $\varepsilon _{t}^{1}$

* The matrices $D_{i}$ contains the response of the variables to the $\varepsilon _{t}$

* In particular the response of the variable $y_{n}$ to an impulse of size one in $\varepsilon_{m}$ $j$-periods ahead is given by the $(n,m)$*-th* element of $D_{j}$. 

 


* Don't worry too much now because firstly we will usually do this with R and secondly because we are going to practice the calculations by hand at the Lab, but ....



         
3) To obtain the orthogonalised IRF (the $D_{i}$ matrices), the matrices with the IRFs
    
  - First we have to obtain the Cholesky factor P as  $PP^{^{\prime }}=\Sigma _{v}$  
  - Latter, we have to calculate the  $D_{i}=C_{i}P$ matrices
         
3a) By hand  

```{r}
VVu <-summary(our_var)[[3]]   #-- The variance-covariance matrix of the residuals
PP_chol <- t(chol(VVu))       #-- Cholesky factor (matrix P in our terminology)
DD1 <- CC1%*%PP_chol
DD2 <- CC2%*%PP_chol
DD3 <- CC3%*%PP_chol
````  



3b) We have to use the `Psi()` function (Note that the function is Psi(), not the previous Phi() ) function  


```{r}
Psi(our_var, nstep = 3)  #- Returns the estimated orthogonalised(Cholesky) coefficient matrices
````         
 
Let's check our DD3 with the calculations made with the var package. 
 
```{r}
DD3   #---  the orthogonalised (Cholesky) responses 3-steps ahead (4th in vars jargon)
````   
 
 <br>
 
* Let's show again the (Cholesky) IRF, but in the usual form:  

```{r}
irf(our_var, impulse = "dx", response = c("dx", "dn"), boot = FALSE, n.ahead = 6, ortho = TRUE, cumulative = FALSE)
```

<br>
   
* In fact, usually IRF's are shown in graphical form:  

```{r}
plot(irf(our_var))
```


<br>
<br>
<br>


--------------  


## FEVD

<br>

* FEVD : Forecast error variance decomposition   

* Once we have orthogonalised IRF's (the $D_{i}$  matrices ), the FEVD can be easily computed. Let's see how:  

<br>

* The h-step forecast error can be represented as:     


$$y_{T+h}-y_{T+h\mid T}=D_{0}\varepsilon _{T+h}+D_{1}\varepsilon_{T+h-1}+\cdots +D_{h-1}\varepsilon _{T+1}$$

As $\Sigma _{\varepsilon }=I$, the forecast error variance of the k-th component of $y_{T+h}$ is:


$$\sigma _{k}^{2}(h)=\overset{h-1}{\underset{j=0}{\sum }}(d_{k1,j}^{2}+\cdots
+d_{kK,j}^{2})=\overset{K}{\underset{j=1}{\sum }}(d_{kj,0}^{2}+\cdots
+d_{kj,h-1}^{2})$$


where $d_{nm,j}$ denotes the $(n,m)-th$ element of $D_{j}$.

<br>

* The quantity $(d_{kj,0}^{2}+\cdots +d_{kj,h-1}^{2})/\sigma _{k}^{2}(h)$ represents the contribution of the $j$-th shock to the h-step forecast error variance  of variable $k$.  


* Don't worry too much now because firstly we will usually do this with R and secondly because we are going to practice the calculations by hand at the Lab, but ....

<br>

The FEVD are obtained through the IRF. Let's see the FEVD and we will calculate it latter by hand:

<br>

```{r}
fevd(our_var, n.ahead = 6)
plot(fevd(our_var))
```


<br>

####  Looking at the relation among IRF and FEVD   

<br>

- To obtain the IRF's we have to use the `irf()` function

<br>

```{r}
irf(our_var, impulse =  c("dx", "dn"), response = "dx", boot = FALSE, n.ahead = 3, ortho = TRUE, cumulative = FALSE)
```

<br>

- To obtain the FEVD we have to use the `fevd()` function

<br>

**fevd()** function to calculate the FEVD
```{r}
fevd(our_var,  n.ahead = 4)

```




<br>

* To illustrate how to obtain the FEVD by hand, let's first look inside the object our_irfs who will contain the IRF's of our_var:

<br>

```{r}
our_irfs<- irf(our_var,n.ahead=5)  #-- our_irfs will contain all the information about IRF 
#str(our_irfs)                     #--- looking inside the object "our_irfs"
class(our_irfs)    
```

<br>

* With `class(our_irfs)` we have seen that `our_irfs` is an object of class "varirf". Pfaff defined this type of object to allocate space to save the IRF's   


* OK, `our_irfs` is an object of class "varirf, but with `str(our_irfs)` we can look at the internal structure of the object "our_irfs" and we can see that in fact is a list of 13 elements. Too many for our purposes, then let's concentrate only in the first element that contains the true IRF. 


* Let's save the first element of our_irfs to look inside again 

<br>

```{r}
our_IRF1 <- our_irfs[[1]]
str(our_IRF1)
```

<br>

* we can see that `our_IRF1` is a list with 2 elements. The first one `our_IRF1[[1]]` contains the responses of the first variable to the two shocks. The second `our_IRF1[[2]` contains the responses of the second variable to the two shocks


* Let's concentrate only on the movement (or responses) of the first variable (dx). The FEVD of the first variable is calculated as:

<br>
 
```{r, results = 'hold'}
dx_e1 <- our_IRF1[[1]][,1]   #-- effect of the 1st shock in the first variable (dx) 
dx_e2 <- our_IRF1[[2]][,1]   #-- effect of the 2nd shock in the first variable (dx)
d2x_e1 <- dx_e1**2                   #-- IRF to the square  (why?)
d2x_e2 <- dx_e2**2                   #-- IRF to the square
ac_d2x_e1 <- cumsum(d2x_e1)          #-- accumulating the squared IRF (why?)
ac_d2x_e2 <- cumsum(d2x_e2)          #-- accumulating the squared IRF
fevd_dx_e1 = ac_d2x_e1/(ac_d2x_e1+ac_d2x_e2)  #-- % of the FEV explained by the 1st shock
fevd_dx_e2 = ac_d2x_e2/(ac_d2x_e1+ac_d2x_e2)  #-- % of the FEV explained by the 2nd shock
``` 
 <br>
 
* Let's see, step by step, what we have done: 

<br>

```{r}
dx_e1[1:5]       #-- effects of 1st shock in the first variable (dx)
dx_e2[1:5]       #-- effects of 2nd shock in the first variable (dx)
d2x_e2[1:5]      #-- IRF to the square 
ac_d2x_e2[1:5]   #-- accumulating the squared IRF
``` 

<br>
<br>


 * Checking if we have calculated the FEVD correctly:
 
 <br>
 
```{r}
fevd_dx_e1[1:4]
fevd_dx_e2[1:4]
fevd(our_var, n.ahead = 4)
```

<br>


- OK, we know already how to estimate-validate-use a VAR in reduced form & to obtain and interpret the Cholesky IRF & FEDV.

<br>

- In the next Lab we will illustrate other ways to identify a SVAR
