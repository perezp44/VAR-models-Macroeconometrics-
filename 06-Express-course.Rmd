---
#title: "Express mini-course on Structural VAR models"
output:
  html_document:
    theme: united
    highlight: monochrome 
    number_sections: no
    toc: true
    toc_float: true
    code_download: true
    #code_folding: show
    #self_contained: TRUE
---



```{r chunk_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, 
                      cache = FALSE, cache.path = "/caches/", comment = "#>",
                      #fig.width = 7, fig.height= 7,   
                      #out.width = 7, out.height = 7,
                      collapse = TRUE,  fig.show = "hold",
                      fig.asp = 7/9, out.width = "60%", fig.align = "center")
```


<style>
div.blue { background-color:#e6f0ff; border-radius: 4px; padding: 13px;}
div.steel { background-color:#dece3c; border-radius: 4px; padding: 13px;}
2c67bf
</style>



```{r options_setup, echo = F}
options(scipen = 999) #- para quitar la notacion cientifica
```

<br><br><br>

As finally, instead of 5 session we are going to have almost 3 sessions, then instead of a mini-course we are going to have a express-mini-course: a quick and **dirty** introduction to the basics of structural VAR models.

# 0. Intro. 

Imagine you are a researcher, or better, imagine that you are the the Economy Minister of a European country, for instance Spain, and you need to know how your country is influenced by economic external forces. We are going to use an statistical approach to try to solve the problem.That is we are going to use a statistical model. But which one? VAR models of course, this is the subject of the mini-course. 

OK, but the original question ("how your country is influenced by economic external forces?") is too wide to be tractable. Let's made the question more specific. We have heard that there exists an international cycle and an European cycle, then the question is how Spain reacts to the international and European cycle. What are their influence in the Spanish economy.

Even more precisely, we are going to try to answer two questions:

  1. What will happen to Spanish economy if something positive (a positive shock) happens in the international cycle or in the European business cycle. (IRF)

  2. What is the importance of international and European cycle in the Spanish economic fluctuations? (FEVD)


OK, we are going to use statistical models, then we will need data. But wait, we already have data, last class we have downloaded some variables. We have downloaded from Eurostat a bunch of series. In particular we are going to use 3 series of "GDP volume" for USA, E15 and our country, in that case Spain.

<br>
<div class = "blue">
**To think:** we will use 3 variables. Are they enough to answer the two questions or we are going to need more  variables/information?
</div>
<br>

We are going to use a statistical approach and we have 3 variables. We can think that the 3 variables are a system where each variable influence the others others, they are interdependent. For instance, changes in US (as a proxy of the international cycle) will impact Spain and Europe. Changes in Europe will also impact on the international economy and of course in the Spanish economy. That is, I think we should treat the 3 variables as endogenous. 


Then ... what type of statistical models we could use? For instance we could use "Simultaneous equations". But wait, those systems were criticised by Sims in 1980 because they use that he called  "incredible identification restrictions".

OK, let's follow Sims's 1980 proposal and use VAR models. First, before to estimate the VAR model, we have to specify, to write it down.




# 1. Specification & estimation

Well, a VAR model is really "easy" to specify. You only need to select a bunch of variables to answer your questions. We have already done that. We have selected 3 variables, and, following Sims's proposal, we are going to:

>  treat all variables as endogenous (and first estimate an unrestricted model in a reduced form). No prior knowledge is used except to decide which variables should enter the system.

<br>
<div class = "blue">
**To think:** in the last sentence there is a tricky part "first estimate an unrestricted model in a reduced form". Try to understand it. What does mean? It's not easy if you don't have the right background. We will try at class.
</div>
<br>



Our model is described by this set of equations: 



$$\left\{ 
\begin{array}{c}
US_{t}= \beta US_{t-1} + \beta US_{t-2} +  \beta E15_{t-1} +  \beta E15_{t-2} +  \beta ES_{t-1} + \beta ES_{t-2} + v_{t}^{US}
\\ 
E15_{t}= \beta US_{t-1} + \beta US_{t-2} +  \beta E15_{t-1} +  \beta E15_{t-2} +  \beta ES_{t-1} + \beta ES_{t-2} + v_{t}^{E15}
\\ 
ES_{t}= \beta US_{t-1} + \beta US_{t-2} +  \beta E15_{t-1} +  \beta E15_{t-2} +  \beta ES_{t-1} + \beta ES_{t-2} + v_{t}^{ES}
\end{array}%
\right\} $$

<br>

But, it's better to write the model in a more compact form:

$$y_{t}~=A_{1}y_{t-1}+A_{2}y_{t-2}+v_{t}$$ 

The model has only two lags, but in general the VAR would be:


$$y_{t}~=A_{1}y_{t-1}+A_{2}y_{t-2}+\ \ ...\ \
+A_{p}y_{t-p}+v_{t}\ \ \ \ \ \ \ \ \ \ [2]$$ 


Model $[2]$ can be written using a polynomial in the lag operator as: 

$$A(L)y_{t}=v_{t}\ \ \ \ \ \ \ \ \ \ [3]$$  

with $A(L)=(I_{K}-A_{1}L^{1}-A_{2}L^{2}-...-A_{p}L^{p})$

<br>

OK, but there is something that we have to solve: we have to choose **how many lags** will have our model, 2, 3 , 4? OK,  but this will be in a moment, first we have to talk a little about the estimation method we are going to use to estimate our model.


Well, you should know that, **if the system is stationary** then, as all the equations in the VAR share the same set of regressors and all of them are lagged variables, the VAR could be estimated efficiently by OLS for each equation separately. 

BUT, be aware that, in order to estimate the system by OLS is crucial that the error terms ($v_{t}$) should **NOT to be autocorrelated**.

Remember also that in the context of VAR methodology, usually the error terms ($v_{t}$) are called **innovations**.



Besides that, if the innovations are normally distributed (Gaussian) then we could apply usual inference methods, like t-test.



<br>
<div class = "blue">
**To think:** VAR models assume $v_{t}\rightarrow N(0,\Sigma _{v})$. Do you think there are contemporaneous correlation among the innovation? Do you think the innovations are autocorrelated? Be aware that they are completely different questions
</div>
<br>



In plain English: before to use a VAR models we have to check if the system of variables is stationary, then, as all of the regressors are in the past $y_{t-k}$, they are "like-exogenous" variables and then we could estimate the parameters of the model by OLS, **BUT** remember that the $v_{t}$ should shown **no autocorrelation**; that is we should check, after estimation, that the model residuals $\widehat{v}_{t}$ shown NO-autocorrelation

Besides that, if the innovations  $(v_{t})$  follow a Gaussian distribution then we could use the usual inference methods. Then, we should check also,the normality of the residuals $(\widehat{v}_{t})$.


<br>

OK. **Let's practise this in R**:

- First we have to load the packages:


```{r}
library("tidyverse")
library("forecast")
#library("dygraphs")
library("vars")         #- the package used to estimate VAR models
```


- Second, we have to load and clean the data

```{r}
# load the data for Spain ---------------------------------------------------
df <- read_csv(here::here("datos", "my_data_ES.csv"))  #- You should load YOUR DATA

#- we are going to use only 3/4 the variables: Vol, Vol_15, Vol_us & time
data <- df %>% dplyr::select(time, Vol_us, Vol_15, Vol)
```


- "Cleaning" the data

```{r}
#- some data munging to obtaining the common sample of the 3 variables ----------------------------------
#- first getting rip off the NAs
data <- data %>% drop_na() 

#- second, calculating the start and end of the common sample of my data:
data_tt <- data %>% mutate(time = as.character(time)) %>% tidyr::separate(time, c("year", "quarter", "day"),"-")
data_tt <- data_tt %>% mutate(year = as.numeric(year)) 
data_tt <- data_tt %>% mutate(quarter= case_when(
                                       quarter  == "01"  ~ 1,
                                       quarter  == "04"  ~ 2,
                                       quarter  == "07"  ~ 3,
                                       quarter  == "10"  ~ 4))
start_year    <- data_tt$year[1] 
start_quarter <- data_tt$quarter[1] 
end_year      <- data_tt$year[nrow(data_tt)] 
end_quarter   <- data_tt$quarter[nrow(data_tt)] 
  
#- we don't need "data_tt" anymore
rm(data_tt)
```



```{r}
#  creating the time series with ts(): -------------------------------------------------------------
Vol    <- ts(data$Vol,    start = c(start_year, start_quarter) , end = c(end_year, end_quarter) , frequency = 4)   
Vol_15 <- ts(data$Vol_15, start = c(start_year, start_quarter) , end = c(end_year, end_quarter) , frequency = 4)   
Vol_us <- ts(data$Vol_us, start = c(start_year, start_quarter) , end = c(end_year, end_quarter) , frequency = 4)   
rm(start_year, start_quarter, end_year, end_quarter)

# taking logs of the series ------------------------
lVol    <- log(Vol)
lVol_15 <- log(Vol_15)
lVol_us <- log(Vol_us)

# taking first log differences --------------------------
dlVol     <- diff(lVol,      lag = 1, difference = 1)
dlVol_15  <- diff(lVol_15,   lag = 1, difference = 1)
dlVol_us  <- diff(lVol_us,   lag = 1, difference = 1)


# putting our 3 variables in a vector
variables <- cbind(dlVol_us, dlVol_15, dlVol)        #- Creating a matrix with the 3 series. The Ordering will be important
```



<br>


- third, We start to specify/estimate our VAR

```{r, eval = FALSE}
# To estimate the VAR we are going to use the "vars" package

# Internal help from the vars package
help(package = vars)           #- calling to the pkg help
ls("package:vars", all = TRUE) #- all the objects, usually functions & data in the "vars" package
```


### 1.a Checking the stationarity of the variables


- Why we need to check the stationarity?

```{r}
library(forecast)
ndiffs(dlVol, alpha = 0.05, test = c("kpss", "adf", "pp"), max.d = 2 )
ndiffs(dlVol_15, alpha = 0.05, test = c("adf"), max.d = 2 )
ndiffs(dlVol_us, alpha = 0.05, test = c("adf"), max.d = 2 )

rm(list = ls()[!ls() %in% c("data", "variables")])   #- removing all except what we need ("data" and "variables")
```


Are the variables stationary? Well ....


### 1.b How many lags?

We have to decide how may lags will our VAR have. There are two strategies:   

      - 1 Use information criteria methods      
    
      - 2 LR tests     
  


- 1. Using information criteria methods 

```{r}
# HOW MANY LAGS in the VAR? What about the deterministic components of the VAR? -------------------

# 1. with model selection criteria (AIC)
VARselect(variables, lag.max = 8, type = "const")
```


<br>
<div class = "blue">
**To think:** If we use the information criteria methods, How many lags we should use in our VAR?
</div>
<br>


2. (Selecting the VAR order) With a sequence of LR test

```{r, eval = FALSE}
# 2. with LR test 
var.m4 <- VAR(variables, p = 4, type = "const") 
var.m3 <- VAR(variables, p = 3, type = "const")   
lrtest(var.m3, var.m4)  #- why doesn't work this test? Why doesn't run this chunk
```


```{r}
var.m5 <- VAR(variables, p = 5, type = "const") 
variables2 <- variables[-1,]
var.m4 <- VAR(variables2, p = 4, type = "const")   
lrtest(var.m4, var.m5)   #- what we select 4 or 5 lags?

rm(var.m4, var.m5, variables2) 
```

<br>
<div class = "blue">
**To think:** How many lags? five, four? 3?
</div>
<br>

Well, finally we are going to use ....... **4** lags. Why? Don't think too much why. One part of the reason is because I like the 4's. I will explain at class

<br>

# 2. Estimation of the VAR

We could estimate the equations of the model by OLS with the function `lm()`, but it's more convenient to use the function `VAR()` from the vars package

```{r}
# ESTIMATING the VAR.  Wchich estimation method? -------------------
VAR(variables, p = 4, type = "const") 
```

<br>


Let's estimate again, but instead of showing the results, we are going to assign them to the `our_var` object:

```{r}
our_var <- VAR(variables, p = 4, type = "const")   #- saving the estimation results in the object "our_var"
```


<br>
<div class = "blue">
**To DO:** please take a look at the "our_var" object. What it is? What it contains?
</div>
<br>



In fact, now you can check if our estimated VAR model is stable:

```{r}
#------ Checking stability(eigenvalues of the companion coefficient matrix must have modulus less than 1)
roots(our_var, modulus=TRUE) #- Returns a vector with the eigenvalues
```



Let's look at the estimation results in two different ways, using two functions of the `vars`package:


```{r}
# LOOKING at the results of the estimated VAR ----------------
summary(our_var)
```



<br>
<div class = "blue">
**To DO:** Please take a look at the results.  Please have another more close look at the Covariance matrix of residuals. **The residuals are contemporaneously correlated?** They are autocorrelated?
</div>
<br>

<br>

Another look at the results of the VAR estimation, now using the `Acoef()`function of the `vars` package:

```{r}
#------ shows the A estimated matrices. Be aware that they are the same results
Acoef(our_var)   
```


<br>
<div class = "blue">
**To Think:** What are this results? Are the same results previously seen or are other results? Why we need this matrices?. (the first two questions are obvious, but not the third)
</div>
<br>



Before to use our estimated VAR we should check if the residuals are correlated (autocorrelation) 


```{r}
serial.test(our_var, lags.pt = 8 )  #-- 8 lags for autocorrelation portmanteau test
```


We have to check also for Normality of residuals.



```{r}
normality.test(our_var, multivariate.only=FALSE)   #-- Normality test
```

OK, more or less, our estimated VAR model is "valid", then we could now use our estimated VAR to perform some task,for instance to test for Granger causality or for forecasting:


```{r, eval = FALSE}
#------ We can test Granger causality
causality(our_var, cause = c("dlVol"))      #- dlVol Granger-cause the other(s) variables?
causality(our_var, cause = c("dlVol_15"))   #- dlVol_15 Granger-cause the other(s) variables?
causality(our_var, cause = c("dlVol_us"))   #- dlVol_15 Granger-cause the other(s) variables?

#------ We can use the VAR for forecasting
predict(our_var, n.ahead = 3)  
fanchart(predict(our_var))
```

But our objective is not to forecast, it's in some way related, but in reality is a little bite different.


# 3. Obtaining the VMA representation

OK. We have estimated a VAR model .... BUT we have not answered the two questions we want to ask: 

  1. What is the response of the Spanish economy to a shock in the international cycle or in the European business cycle. (IRF)

  2. What is the importance of international and European cycle in the Spanish economic fluctuations? (FEVD)
  
<br>

In order to answer those questions we need to work a little bite more. First we have to obtain the VMA representation of our VAR model. That is, we have to "invert" our model. How and why? The idea is the same that Mariam explained to you, something like:

> every stationary AR process have a MA representation


As we are now with VAR (vectorial-AR) we would obtain a vectorial-MA (VMA), but the idea is the same. Let's do it (by hand) for a VAR with only 1 lag:




```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics(here::here("imagenes", "01-horizontal-a.jpg") ) 
```

Finally we have that our vector of variables $y_{t}$ as a function of the innovations in $t$, in $t-1$, $t-2$ ....   This is the VMA representation. The variables in function of actual and past values of innovations ($v_{t}$).

Usually we rename the matrices to call them $C_{i}$ and the VMA that you see in papers and books usually is written like:

$$y_{t}~=C_{0}v_{t}+C_{1}v_{t-1}+C_{2}v_{t-2}+\ ...\  [3]$$



being $C_{0}=I_{K}$, the rest of the $C_{i}$ are computed recursively from the VAR representation , that is they are function of the $A_{i}$ matrices.


As with the VAR, model $[3]$ can be written using a polynomial in the lag operator as:
 

$$y_{t}=C(L)v_{t}\ \ \ \ \ \ \ \ \ \ \ \ [4]$$

being $C(L)=(I_{K}+C_{1}L^{1}+C_{2}L^{2}+\ \ ...)$.


OK, but why we need this new representation of our system $y_{t}$. For different reason, but in the context of VAR models, because the VMA representation, that is, the sequence of $C_{i}$ matrices provide the IRF^[We will see in short that this is not really true because ....], that is, this sequence of $C_{i}$ provide the answer to our first question. 


For instance, in our example, Imagine that:


$$y_{t}=\left[ 
\begin{array}{c}
USA_{t} \\ 
U15_{t} \\ 
ESP_{t}%
\end{array}%
\right] \ \ \ \ v_{t}=\left[ 
\begin{array}{c}
v_{t}^{USA} \\ 
v_{t}^{E15} \\ 
v_{t}^{ESP}%
\end{array}%
\right] $$



$$y_{t}~=C_{0}v_{t}+C_{1}v_{t-1}+C_{2}v_{t-2}+C_{3}v_{t-3}+C_{4}v_{t-4}+...$$

$$y_{t}~=\left[ 
\begin{array}{cc}
1.0 & 0.0 & 0.0 \\
0.0 & 1.0 & 0.0 \\
0.0 & 0.0 & 1.0%
\end{array}%
\right] v_{t}+\left[
\begin{array}{cc}
1.1 & 1.2 & 1.3 \\ 
1.4 & 1.5 & 1.6 \\
1.7 & 1.8 & 1.9%
\end{array}%
\right] v_{t-1}+\left[
\begin{array}{cc}
2.1 & 2.2 & 2.3 \\ 
2.4 & 2.5 & 2.6 \\
2.7 & 2.8 & 2.9%
\end{array}%
\right] v_{t-2}+\left[
\begin{array}{cc}
3.1 & 3.2 & 3.3 \\ 
3.4 & 3.5 & 3.6 \\
3.7 & 3.8 & 3.9%
\end{array}%
\right] v_{t-3}+...$$





<br>
<div class = "blue">
**TO DO:** Track what are going to be the effect in 2020, 2021, 2022 & 2023 in the US Vol_GDP if a innovation of size 1 in $v^{USA}$ happens today(2020). That is I'm asking you to calculate what will be the effect thought time (t, t+1, t+2 ...) on USA of an innovation of size 1 in his own equation, that is in $v_{t}^{USA}$ 
</div>
<br>

The answer is: 1, 1.1, 2.1, 3.1 ....

<br>
<div class = "blue">
**TO DO:** Track what are going to be the effect **on Spain** in 2020 2021, 2022 & 2023 if today (2020) there is an innovation of size 1 in  $v^{E15}$. That is I'm asking you to calculate what will be the effect thought time on Spain of an innovation of size 1 in $v^{E15}$. Please, make an effort, it is really important to notice and to understand this.
</div>
<br>


Then, we have solved already our first question, that is, in order to to estimate what the effects of USA & E15 in the Spanish economy we have first to estimate a VAR and then, to invert the polynomial to obtain the sequence of $C$ matrices, that is the $C(L)$ polynomial. 

And, that's all? Do you think things are so easy? Obviously No, it's only part of the process, we will continue explaining this latter.


Now, let's obtain the VMA representation of our_var with R.

```{r}
#------ Obtaining the Wold VMA representation of our_var with the vars package
Phi(our_var, nstep = 3)  #- shows the WOLD VMA  estimated matrices. (by inverting the A's matrices)
```




<br>
<div class = "blue">
**TO DO:** Track what are the **estimated** effect of an US "shock" in the Spanish economy.
</div>
<br>


To finish the second class. In fact the first class of the express course we have to understand why the Wold VMA is **NOT USEFUL**.


Why it's not useful the $y_{t}~= C(L)v_{t}= C_{0}v_{t}+C_{1}v_{t-1}+C_{2}v_{t-2}+C_{3}v_{t-3}+C_{4}v_{t-4}+...$?


The why is in the variance-covariance matrix of the innovations $\Sigma _{v}$, this matrix it's usually not diagonal. That it's the innovations are contemporaneously correlated and we think, we believe, that a true structural shock should be "independent", orthogonal to other structural shocks.

In a more formal way:

> BUT, usually the VAR disturbances (or innovations) are correlated, so the interpretability of the impulse responses to innovation becomes problematic: if the innovations are correlated (off diagonal elements of $\Sigma _{v}$ different from zero) then, an impulse in for instance, $v_{USA}$ would be associated with impulses in innovations in the other equations of the VAR model. In other words, as the innovations are not likely to occur in isolation, then tracking the effect of an innovation in isolation does not reflect what actually happens in the system after an innovation hits the system.

<br>

# 4. Structural VARs

OK, we can estimate a VAR an inverting it we can obtain something "close" to the IRF's we want but they are not really structural because the innovation are correlated, then they are not really structural shocks.

To interpret the VAR in an economically meaningful way, one needs to "transform" the vector of innovations($v_{t}$) into "structural" shocks ($\varepsilon _{t}$), like monetary policy shocks, productivity shocks, ... and ideally the structural shocks should be:  1) orthogonal shocks  2) shocks with economic meaning. 

That's now our objective to transform our system in a way that our variables, the $y_{t}$ instead of being function of the innovations ($v_{t}$), they were function of structural shocks ($\varepsilon_{t}$).


Remember, that we can obtain easily the (Wold) VMA representation:  $y_{t} =  C(L)v_{t}$ where $\Sigma _{v}$   is not the identity, then the innovations are correlated. BUT we would like to obtain  something like $y_{t} = D(L)\varepsilon_{t}$ where  $\Sigma _{\varepsilon }=I$


Transforming the system from one equation to the other is in fact a mathematical problem, an C. A. Sims is a mathematician. In fact their first proposal was: 


>  The structural VAR methodology first estimate an unrestricted model in a reduced form. No prior knowledge is used except to decide which variables should enter the system.

> Sims’s original idea to obtain IRF & FEVD was to assume recursive contemporaneous interactions among variables, i.e. by imposing a certain structural ordering of the variables. In terms of the moving average (MA) representation, the structural shocks do not affect preceding variables simultaneously. In fact he proposed to use the Cholesky decomposition.

Let's try to explain this last cryptic sentence. 

To obtain/estimate an structural VAR, the usual procedure is:

- first a (reduced form) VAR model is estimated

- we could invert the VAR to obtain the (Wold) VMA but it's not really useful. We can obtain (1), but we want a structural model (2)

- from the Wold or reduced form VMA representation we will obtain the structural VAR. 


<br>

In a diagram: 


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics(here::here("imagenes", "02-VMA.jpg") ) 
```

<br>

Equation (2) will be the structural VAR, in fact is, more precisely the structural VMA representation for our our system of variables $y_{t}$ , that we want to obtain to answer our two questions.

<br>

## 4.1 Sims's first proposal

The first proposal to "recover" the structural information from the reduced form estimates was made by Sims in 1980.

In particular, Sims(1980) propose to use the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) to obtain (2) from (1). Let's develop it a little:

- Cholesky found that every symmetric definite positive matrix, let's call it $M$ admits a **unique** factorisation like $M = PP^{^{\prime }}$. Where  $P$ is called the Cholesky factor and it is a  **lower triangular** matrix. 

- Sims is a mathematician and he obviously knew the Cholesky decomposition, then he proposed in his 1980 paper to use this Theorem in order to transform our reduced form VAR to a "structural" one. Structural in the sense that the transformed "innovations" were orthogonal. Let's explain this a little bite more:

- The variance-covariance matrix of our VAR, the $\Sigma _{v}$, as it is a variance-covariance matrix, it admit the Cholesky decomposition: $\Sigma _{v} = PP^{^{\prime }}$, where $P$ is the Cholesky factor; that is $P$ matrix has some properties:  

  1)  $P$ is lower triangular, and   
  2)  $P^{-1}\Sigma _{v}P^{{-1}^{\prime}}= I$  

Then, Sims proposes to transform the reduced form VMA representation (1), like this:

- $y_{t} =  C(L)v_{t}$


- $y_{t}= C(L) PP^{-1}v_{t}$

It seems stupid, because it seems that we are complicating things, but let's concentrate on a specific part of the last equation: $P^{-1}v_{t}$. 

- In fact, $P^{-1}v_{t}$ is a transformation/rotation of the innovations. We are multiplying, we are transforming, the innovations, the $\v_{t}$, and then obtaining a new vector of "innovations". We can call those new innovations as we want, for instance $P^{-1}v_{t} = \varepsilon_{t}$


Let's see what are the covariance matrix of these new "innovations" that we have called $\varepsilon_{t}$. Do you guess?


```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics(here::here("imagenes", "03-VMA.jpg") ) 
```

<br>

That is, the "transformed innovations" are orthogonal .... then Sims interpreted them as **structural shocks**. Good, BUT his proposal received some critiques ... that we will see latter.

<br> 

By now let's return to: 

- $y_{t}= C(L) PP^{-1}v_{t}$    as we have call $P^{-1}v_{t} = \varepsilon_{t}$, in fact we have:


- $y_{t}= C(L) P\varepsilon_{t}$   



<br>
<div class = "blue">
**TO THINK:** The last equation $y_{t}= C(L) P\varepsilon_{t}$   is a structural model? Could it be used to obtain the answers to our two questions (the importance of the European shock into the Spanish economy)? How can we obtain this new representation of $y_{t}$, the so called structural VAR model claimed by Sims?
</div>
<br>


In fact we can call $C(L)P = D(L)$ and we will have $y_{t}= D(L) \varepsilon_{t}$, and the $D(L)$ will give us the response of the variables of our system ($y_{t}$) to a vector of shocks that are orthogonal ($\varepsilon_{t}$).


<br>
<div class = "blue">
**TO THINK:** How we can calculate the D(L)
</div>
<br>


Think that $C(L)P = D(L)$; that is, 

$$(C_{0} + C_{1}L + C_{2}L^{2}) P = D_{0} + D_{1}L + D_{2}L^{2}$$

That is : 

- $C_{0} P = D_{0}$

- $C_{1} P = D_{1}$ ...


<br>

That's was the proposal of Sims to estimate structural IRF's: it consists on first to estimate a VAR (in reduced form), invert the autoregressive polynomial to obtain the (reduced form) VMA representation for $y_{t}$, an finally to use the Cholesky factor to transform the reduced form VMA to a "structural" representation.  In more technical words:

> Sims’s original idea was to obtain IRF&FEVD was to assume recursive contemporaneous interactions among variables, i.e. by imposing a certain structural ordering of the variables. In terms of the moving average (MA) representation, the structural shocks do not affect preceding variables simultaneously (Cholesky)

By now you should start to understand this last sentence. There is still a tricky part there: the "assume recursive contemporaneous interaction" part.

To finally understand the sentence, we should return to the structural VAR representation: 


$$y_{t}= C(L) P\varepsilon_{t} = D(L)\varepsilon_{t}$$

and to remember that $D_{0} = P$ and that $P$ is lower triangular , then ..... when we transform the reduced VMA to obtain the structural VMA at the end we are doing, we are imposing that the matrix that gives the contemporaneous or instantaneous effect of the structural shocks $\varepsilon_{t}$ it's  in some sense recursive. We are imposing a recursive structural among our 3 equations.

$$y_{t}~=D_{0}\varepsilon_{t}+D_{1}\varepsilon_{t-1}+D_{2}\varepsilon_{t-2}+D_{3}\varepsilon_{t-3}+\varepsilon_{4}v_{t-4}+...$$

The matrix $D_{0}$ contains the contemporaneous/instantaneous response of the variables ($y_{t}$) to the structural shocks $\varepsilon_{t}$. **AND**, how it is $D_{0}$?



<br>
<div class = "blue">
**TO THINK:** How it is $D_{0}$? Lower triangular,  Why? and, what does implies that $D_{0}$ is lower triangular?
</div>
<br>


If $D_{0}$ is lower triangular, that means that an "innovation" (in fact now, a structural shock , $\varepsilon{t}$) in the first equation could **contemporaneously** affects to **ALL** the variables in the VAR, while the innovation in the second equation could contemporaneously affect all the variables in the VAR **except** the first one, … and finally, an innovation in the last equation could contemporaneously affects only the last variable in the VAR



## 4.2 Critiques to Sims's proposal

The proposal of Sims's 1980 implies that the first variable in the system, in our case the GDP of USA could affect all the variables in the system. The second variable, in our case E15 GDP could affect only the second and third variable. And our third variable (Spanish GDP) could only affect his own dynamics contemporaneously. That is, when we transform our VAR using the Cholesky decomposition we are imposing a recursive structure among the variables. The order of variables is important, because only the first variables can affects all the others variables contemporaneously.



<br>
<div class = "blue">
**TO THINK:** Do you think that it is sensible to impose a recursive structure in our system? The answer is maybe yes, maybe not ...
</div>
<br>



Of course the proposal of Sims received some critiques as well. In fact one of the claims of Sims in favour of using VAR is that their use avoid to makes unnecessary the use of what Sims called "incredible restrictions" that other approaches, like for instance simultaneous equations models,  used.

The first proposal (Sims) to identify the structural form by means of the Cholesky decomposition. It's seems that he it's not impossing none restriction but ...

     
* Using the Cholesky factorization to identify the structural VAR is equivalent to choosing a recursive system of equations. **BUT**, as you can imagine, the order matters; that is, **identification is not unique**.


<br>
<div class = "blue">
**TO THINK:** What does mean **identification is not unique**? Could yo explain it with an example using our 3 variables?
</div>
<br>


* It is important to keep in mind that the "orthogonalization" of the reduced-form residuals by applying a Cholesky decomposition is appropriate only if the recursive structure embodied in the Cholesky factorisations can be justified on economic grounds.

* Cooley and LeRoy (1985) criticized the VAR methodology because of its "atheoretical" identification scheme. They argued that Sims did not explicitly justify the identification restrictions and claimed that a model identified by this arbitrary procedure cannot be interpreted as a structural model, because a different variable ordering yields different structural parameters. 

* Sims (1986) propose trying different orderings (there are n!) and checking if the results are robust. In general, the higher the elements off-diagonal elements of $\Sigma _{v}$ are, the higher the changes in the results.

* But, even if there were no differences across these n! specifications, this would only prove that the results are robust among all recursive orderings, but there is no reason for the model to be recursive in the first place.

* Since then, several ways to identify VAR models have been proposed (short-run restrictions, long-run restrictions, cointegration restrictions, sign restriction, narrative approaches etc. ).

* As an alternative to the recursive identification scheme, Bernanke (1986) and Blanchard and Watson (1986) among others introduced non-recursive restrictions on the contemporaneous interactions among variables for identification

* As economic theory often does not provide enough meaningful contemporaneous restrictions (and the more variables you put into your system, the more restrictions you need), the search for additional identifying restrictions led Blanchard and Quah (1989) and subsequently Shapiro and Watson (1988) and Gali (1992) to introduce restrictions on the system's long-run properties. These long run restrictions are usually based on neutrality postulates

* Faust and Leeper (1997) have criticized the use of long run restrictions to identify structural shocks, and show that unless the economy satisfies some types of strong restrictions, the long run restrictions will be unreliable

* More recently, imposing sign restrictions, allows you to test the implications of all types of restrictions. By dropping one after one of the "dubious restrictions", one can test whether the responses to shocks are sensitive to the restrictions often imposed

* For a detailed exposition and examples of different sources of identifying restrictions see [Kilian(2011)](https://ideas.repec.org/p/cpr/ceprdp/8515.html)



Remember: a structural VAR analysis usually start specifying, validating and finally estimating a VAR in reduced form. For the VAR representation we could obtain a reduced form VMA representation, but this reduced VMA is has little interest because it's not structural, their IRF's have not a clear interpretation because the innovations ($v_{t}$) are not orthogonal, they are correlated. Then, in the structural VAR methodology the reduced VMA representation is rarely show, instead a way to recover, to identify the structural VAR is proposed.


Usually this implies the use of some restrictions. The first proposal made by Sims in 1980 implied to impose a recursive system. 

- The  most common restrictions are short-run restriction or long-run restrictions (restriction on $D(1)$). I will explain this at class but you soul try to understand the slides or to read some material. 


- In fact in class we will use or three variables to estimate a structural VAR, but we will use for pedagogical reasons two ways to identify the shocks, one only with short run restriction , that is restriction in $D_{0}$, and the other one using only restriction on the long-run, that is in the $D(1)$ matrix.

- We will see this at class in the Lab section.



<br>
<div class = "steel">
**ATTENTION:** We have reached almost the end of the **express**-mini-course, BUT at least you should know the second most used "instrument" in VAR methodology: the FEVD
</div>
<br>



## 4.3 IRF and FEVD

Once we have identified/obtained our structural VAR you could obtain the IRF and FEDV that are the two principal instruments of the VAR methodology in order to show their results.

The IRF is nothing more that a graphical representation of the sequence of $D_{i}$ matrices. This matrices contains the information we are interested in, the effect thought time of the structural shocks $\varepsilon_{t}$ into the variables $y_ {t}$


As an example to illustrate this, imagine that our system would be:


$$y_{t}=\left[ 
\begin{array}{c}
Y_{t} \\ 
P_{t}%
\end{array}%
\right] \ \ \ \varepsilon _{t}=\left[ 
\begin{array}{c}
\varepsilon _{t}^{1} \\ 
\varepsilon _{t}^{2}%
\end{array}%
\right] $$

<br>

An that we have obtained/identified the structural VAR using the Cholesky decomposition, then imagine that the structural VAR $y_{t} = D(L) \varepsilon_{t}$ is:


$$y_{t}~=D_{0}\varepsilon _{t}+D_{1}\varepsilon
_{t-1}+D_{2}\varepsilon _{t-2}+D_{3}\varepsilon _{t-3}+D_{4}\varepsilon
_{t-4}+...$$



$$y_{t}~=\left[ 
\begin{array}{cc}
0.9 & 0.8 \\ 
0.7 & 0.6%
\end{array}%
\right] \varepsilon _{t}+\left[ 
\begin{array}{cc}
0.5 & 0.4 \\ 
0.3 & 0.2%
\end{array}%
\right] \varepsilon _{t-1}+\left[ 
\begin{array}{cc}
0.1 & -0.1 \\ 
-0.2 & -0.3%
\end{array}%
\right] \varepsilon _{t-2}+\left[ 
\begin{array}{cc}
-0.4 & -0.5 \\ 
-0.6 & -0.7%
\end{array}%
\right] \varepsilon _{t-3} +...$$

<br>

- It would be possible that the element (1,2) of $D_{0}$ were 0.8? The answer it's NO



<br>
<div class = "blue">
**TO THINK:** It would be possible that the element (1,2) of $D_{0}$ were 0.8? The answer it's NO, but **WHY?**  Remember that were are using Cholesky decomposition and this implies that ...... *lower triangular* ....
</div>
<br>


How we can obtain the IRF's of our VAR? We will see at the Lab but, with the `vars::Psi()` function you can obtain the $D_{i}$, the structural matrices (using the Cholesky decomposition)


```{r}
Psi(our_var, nstep = 2)  #- Returns the estimated orthogonalised(Cholesky) coefficient matrices
```


And with the `vars::irf()` function we obtain the same information but in a more convenient way: we obtain the IRF


```{r}
irf(our_var, n.ahead = 4, boot = FALSE)
```


WE can be more specific and only to demand the IRF we are interested in:

```{r}
irf(our_var, n.ahead = 5,
             impulse = "dlVol_us", 
             response = c("dlVol", "dlVol_15"), 
             boot = FALSE,
             cumulative = FALSE)
```


But usually you don't show the number but you plot them:

```{r}
my_irf <- irf(our_var, n.ahead = 25,
                       impulse = "dlVol_us", 
                       response = c("dlVol", "dlVol_15"), 
                       boot = FALSE,
                       cumulative = FALSE)

plot(my_irf)
```


<br>

### Finally the FEVD's


- the first idea is that the FEVD's comes from the IRF's


* Once we have orthogonalised IRF's (the $D_{i}$  matrices ), the FEVD can be easily computed. Let's see how:  


* The h-step ahead forecast error can be represented as:     


$$y_{T+h}-y_{T+h\mid T}=D_{0}\varepsilon _{T+h}+D_{1}\varepsilon_{T+h-1}+\cdots +D_{h-1}\varepsilon _{T+1}$$

As $\Sigma _{\varepsilon }=I$, the forecast error variance of the k-th component of $y_{T+h}$ is:


$$\sigma _{k}^{2}(h)=\overset{K}{\underset{j=1}{\sum }}(d_{kj,0}^{2}+\cdots
+d_{kj,h-1}^{2})$$


where $d_{nm,j}$ denotes the $(n,m)-th$ element of $D_{j}$.



* The quantity $(d_{kj,0}^{2}+\cdots +d_{kj,h-1}^{2})/\sigma _{k}^{2}(h)$ represents the contribution of the $j$-th shock to the h-step ahead forecast error variance  of variable $k$.  


* Don't worry too much now firstly because we will usually do this with R and secondly because we are going to practise the calculations by hand at the Lab, but ....



* As an example to illustrate:


$$y_{t}=\left[ 
\begin{array}{c}
Y_{t} \\ 
P_{t}%
\end{array}%
\right] \ \ \ \varepsilon _{t}=\left[ 
\begin{array}{c}
\varepsilon _{t}^{1} \\ 
\varepsilon _{t}^{2}%
\end{array}%
\right] $$



$$y_{t}~=D_{0}\varepsilon _{t}+D_{1}\varepsilon
_{t-1}+D_{2}\varepsilon _{t-2}+D_{3}\varepsilon _{t-3}+D_{4}\varepsilon
_{t-4}+...$$



$$y_{t}~=\left[ 
\begin{array}{cc}
0.9 & 0.0 \\ 
0.7 & 0.6%
\end{array}%
\right] \varepsilon _{t}+\left[ 
\begin{array}{cc}
0.5 & 0.4 \\ 
0.3 & 0.2%
\end{array}%
\right] \varepsilon _{t-1}+\left[ 
\begin{array}{cc}
0.1 & -0.1 \\ 
-0.2 & -0.3%
\end{array}%
\right] \varepsilon _{t-2}+...$$



$$...+\left[ 
\begin{array}{cc}
-0.4 & -0.5 \\ 
-0.6 & -0.7%
\end{array}%
\right] \varepsilon _{t-3}+D_{4}\varepsilon _{t-4}+...$$


<br>

- we will practice and understand them at the Lab, but we can calculate them with the `vars::fevd()` function:

```{r}
fevd(our_var, n.ahead = 5) #- with the vars pkg
```

Usually the FEVD's are also shown in  a graphical fashion:


```{r}
plot(fevd(our_var))
```



